from pyexpat import features
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.utils.rnn as rnn_utils

class LSTM_ecg(nn.Module): #LSTM for time series 
    '''
    LSTM Module
    for self-supervised ECG: 2 layers and 512 hidden units
    concat-pooling layer, which concatenates the maximum of all LSTM outputs
    single hidden layer with 512 units including batch normalization and dropout
    '''
    def __init__(self, config):
        super().__init__()
        # params: "n_" means dimension
        self.n_layers = config['n_layers']   # number of layers
        self.config = config
        self.lstm = nn.LSTM(config['n_embed'], config['n_hidden'], num_layers=config['n_layers']
                , bidirectional=config['b_dir'], batch_first=True)
        self.dropout = nn.Dropout(config['fc_drop'])
        self.concat_pool = config.get('concat_pool',False)
        if self.concat_pool:
            self.concat_fc = nn.Sequential([nn.Dropout(config['fc_drop']),
                nn.Linear(3 * config['n_hidden'], config['n_hidden']),
                nn.BatchNorm1d(config['n_hidden']),
                nn.ReLU(),])
        self.fc = nn.Linear(config['n_hidden'], config['n_output'])

    def extract_features(self, texts, seq_lens):
        packed_embedded = nn.utils.rnn.pack_padded_sequence(
            texts, seq_lens.cpu())  # seq_len:128 [0]: lenght of each sentence
        rnn_out, (hidden, cell) = self.lstm(
            packed_embedded)  # bs X len X n_hidden
        if self.concat_pool:
            features = torch.cat([torch.max(rnn_out,dim=1)[0],torch.mean(rnn_out,dim=1),
                        rnn_out[:,-1,:].view(len(seq_lens),self.config['n_hidden'])])
            features = self.concat_fc(features)
        else:
            features = rnn_out[:,-1,:].view(len(seq_lens),self.config['n_hidden'])

        return features

    def classify(self, features):
        fc_out = self.fc(features)  # bs x d_out
        return fc_out

    def forward(self, x, seq_lens):
        x = self.extract_features(x, seq_lens)
        return self.classify(x)

class LSTM_modal(nn.Module): #LSTM for time series 
    '''
    LSTM Module
    for MODALS : n_hidden=128, b_dir=False, n_layers=1
    '''
    def __init__(self, config):
        super().__init__()
        # params: "n_" means dimension
        self.n_layers = config['n_layers']   # number of layers
        # number of hidden nodes
        self.lstm = nn.LSTM(config['n_embed'], config['n_hidden'], num_layers=config['n_layers']
                , bidirectional=config['b_dir'], batch_first=True)
        self.dropout = nn.Dropout(config['fc_drop'])
        self.fc = nn.Linear(config['n_hidden'], config['n_output'])

    def extract_features(self, texts, seq_lens):
        packed_embedded = nn.utils.rnn.pack_padded_sequence(
            texts, seq_lens.cpu())  # seq_len:128 [0]: lenght of each sentence
        rnn_out, (hidden, cell) = self.lstm(
            packed_embedded)  # 1 X bs X n_hidden
        features = hidden.permute(1, 0, 2).reshape(len(seq_lens), -1) # bs X n_hidden
        return features

    def classify(self, features):
        fc_out = self.fc(features)  # bs x d_out
        return fc_out

    def forward(self, x, seq_lens):
        x = self.extract_features(x, seq_lens)
        return self.classify(x)